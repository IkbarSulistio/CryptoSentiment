{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2617525d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\py_packages\\python311\\site-packages (4.53.2)\n",
      "Requirement already satisfied: datasets in c:\\py_packages\\python311\\site-packages (3.5.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\py_packages\\python311\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pandas in c:\\py_packages\\python311\\site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in c:\\py_packages\\python311\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\py_packages\\python311\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\py_packages\\python311\\site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\py_packages\\python311\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\py_packages\\python311\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\py_packages\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\py_packages\\python311\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\py_packages\\python311\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\py_packages\\python311\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\py_packages\\python311\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\py_packages\\python311\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\py_packages\\python311\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\py_packages\\python311\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\py_packages\\python311\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\py_packages\\python311\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\py_packages\\python311\\site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\py_packages\\python311\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\py_packages\\python311\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\py_packages\\python311\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\py_packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\py_packages\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\py_packages\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: colorama in c:\\py_packages\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\py_packages\\python311\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\py_packages\\python311\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\py_packages\\python311\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\py_packages\\python311\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\py_packages\\python311\\site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\py_packages\\python311\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\py_packages\\python311\\site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\py_packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\py_packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\py_packages\\python311\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\py_packages\\python311\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\py_packages\\python311\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\py_packages\\python311\\site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\py_packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\py_packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\py_packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\py_packages\\Python311\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\IKBAR\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets scikit-learn pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07b8e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\py_packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████| 534/534 [00:44<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train Loss: 0.9316 - Val Loss: 0.7534 - Val Acc: 0.6900 - Val F1: 0.6882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 534/534 [00:43<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Train Loss: 0.4835 - Val Loss: 0.5473 - Val Acc: 0.8100 - Val F1: 0.8090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 534/534 [00:45<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Train Loss: 0.2271 - Val Loss: 0.5052 - Val Acc: 0.8300 - Val F1: 0.8301\n",
      "Training time: 152.74 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertConfig\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SequenceClassificationDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs['input_ids'][idx],\n",
    "            'attention_mask': self.inputs['attention_mask'][idx],\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class DistilBertFineTuning:\n",
    "    def __init__(self, dataset_path, train_file, validation_file, feature_col, label_col, model_name, batch_size, learning_rate, num_epochs, max_len, optimizer='AdamW', device='cpu'):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.train_file = train_file\n",
    "        self.validation_file = validation_file\n",
    "        self.feature_col = feature_col\n",
    "        self.label_col = label_col\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.max_len = max_len\n",
    "        self.optimizer_type = optimizer\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        self.train_df = pd.read_csv(os.path.join(self.dataset_path, self.train_file))\n",
    "        self.validation_df = pd.read_csv(os.path.join(self.dataset_path, self.validation_file))\n",
    "\n",
    "        self.num_labels = len(self.train_df[self.label_col].unique())\n",
    "\n",
    "        self.tokenized_train = self.tokenize_dataset(self.train_df, self.feature_col, self.label_col)\n",
    "        self.tokenized_validation = self.tokenize_dataset(self.validation_df, self.feature_col, self.label_col)\n",
    "\n",
    "        self.model_config = DistilBertConfig.from_pretrained(self.model_name, num_labels=self.num_labels)\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained(self.model_name, config=self.model_config)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        if self.optimizer_type == 'Adam':\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        elif self.optimizer_type == 'AdamW':\n",
    "            self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer type. Use 'Adam' or 'AdamW'.\")\n",
    "\n",
    "        self.train_dataloader = self.create_dataloader(self.tokenized_train)\n",
    "        self.validation_dataloader = self.create_dataloader(self.tokenized_validation, shuffle=False)\n",
    "\n",
    "    def tokenize_dataset(self, df, feature_col, label_col):\n",
    "        tokens = self.tokenizer(\n",
    "            list(df[feature_col]),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return tokens, list(df[label_col])\n",
    "\n",
    "    def create_dataloader(self, tokenized_dataset, shuffle=True):\n",
    "        dataset = SequenceClassificationDataset(tokenized_dataset[0], tokenized_dataset[1])\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle)\n",
    "\n",
    "    def evaluate_model(self, dataloader):\n",
    "        self.model.eval()\n",
    "        all_labels, all_predictions = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                inputs = {key: value.to(self.device) for key, value in batch.items()}\n",
    "                labels = inputs[\"labels\"]\n",
    "                outputs = self.model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "                logits = outputs.logits\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "        recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            train_losses = []\n",
    "\n",
    "            for batch in tqdm(self.train_dataloader, desc=f'Epoch {epoch + 1}/{self.num_epochs}'):\n",
    "                inputs = {key: value.to(self.device) for key, value in batch.items()}\n",
    "                outputs = self.model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=inputs[\"labels\"])\n",
    "                loss = outputs.loss\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            validation_losses = []\n",
    "            val_acc, val_prec, val_rec, val_f1 = self.evaluate_model(self.validation_dataloader)\n",
    "\n",
    "            for batch in self.validation_dataloader:\n",
    "                inputs = {key: value.to(self.device) for key, value in batch.items()}\n",
    "                outputs = self.model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=inputs[\"labels\"])\n",
    "                validation_losses.append(outputs.loss.item())\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs} - Train Loss: {sum(train_losses)/len(train_losses):.4f} - Val Loss: {sum(validation_losses)/len(validation_losses):.4f} - Val Acc: {val_acc:.4f} - Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    def save_model(self, directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        self.model.save_pretrained(directory)\n",
    "        self.tokenizer.save_pretrained(directory)\n",
    "        \n",
    "start_time = time.time()\n",
    "model = 'distilbert'\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 3\n",
    "batch_size = 6\n",
    "max_len = 512\n",
    "optimizer = 'AdamW'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "absolute_path = \"D:/Berkas/Code/CryptoNew/\"\n",
    "dataset_path = absolute_path + \"Datasets/\"\n",
    "train_file = 'train_set.csv'\n",
    "validation_file = 'validation_set.csv'\n",
    "feature_col = 'text'\n",
    "label_col = 'sentiment_numerical'\n",
    "trained_model = model + '_optimizer_' + optimizer + '_lr_' + str(learning_rate) + '_epochs_' + str(num_epochs) + '_bs_' + str(batch_size) + '_maxlen_' + str(max_len)\n",
    "\n",
    "classifier = DistilBertFineTuning(dataset_path, train_file, validation_file, feature_col, label_col, model_name, batch_size, learning_rate, num_epochs, max_len, optimizer, device)\n",
    "\n",
    "classifier.train()\n",
    "classifier.save_model(absolute_path + 'TrainedModels/' + trained_model)\n",
    "\n",
    "print(\"Training time: {:.2f} seconds\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927e1b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediksi selesai disimpan ke test_set_distil_adamw.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "# Gunakan path lokal (ganti sesuai komputermu)\n",
    "absolute_path = \"D:/Berkas/Code/CryptoNew/\"\n",
    "test_file = 'test_set_distil_adam.csv'\n",
    "trained_model_name = 'distilbert_optimizer_AdamW_lr_2e-05_epochs_3_bs_6_maxlen_512'\n",
    "\n",
    "# Baca data uji\n",
    "test_df = pd.read_csv(os.path.join(absolute_path, 'Datasets', test_file))\n",
    "\n",
    "# Load model dan tokenizer yang sudah dilatih\n",
    "model_path = os.path.join(absolute_path, 'TrainedModels', trained_model_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Tokenisasi data uji\n",
    "tokenized_test = tokenizer(\n",
    "    list(test_df['text']),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ").to(device)\n",
    "\n",
    "# Prediksi\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = {key: value.to(device) for key, value in tokenized_test.items()}\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    _, predicted_labels = torch.max(logits, dim=1)\n",
    "\n",
    "# Simpan hasil prediksi\n",
    "test_df['distilbert_adamw_ft_prediction'] = predicted_labels.cpu().numpy()\n",
    "test_df.to_csv(os.path.join(absolute_path, 'Datasets', 'test_set_distil_adamw.csv'), index=False)\n",
    "print(\"Prediksi selesai disimpan ke test_set_distil_adamw.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b8a652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8530\n",
      "Precision:  0.8534\n",
      "Recall   :  0.8530\n",
      "F1 Score :  0.8530\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Path ke file hasil prediksi\n",
    "file_path = \"D:/Berkas/Code/CryptoNew/Datasets/test_set_distil_adamw.csv\"\n",
    "\n",
    "# Baca file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ambil label asli dan prediksi\n",
    "y_true = df['sentiment_numerical']\n",
    "y_pred = df['distilbert_adamw_ft_prediction']\n",
    "\n",
    "# Hitung metrik\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(f\"Accuracy :  {accuracy:.4f}\")\n",
    "print(f\"Precision:  {precision:.4f}\")\n",
    "print(f\"Recall   :  {recall:.4f}\")\n",
    "print(f\"F1 Score :  {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
