{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxQ4PrvRYkJC"
   },
   "source": [
    "## Fine-tune FinBERT to make predictions based on specific train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvcc' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 16 13:01:10 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.86                 Driver Version: 551.86         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   46C    P8              3W /   75W |       0MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     14108    C+G   ...inaries\\Win64\\EpicGamesLauncher.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())         # True if CUDA is usable\n",
    "print(torch.cuda.get_device_name(0))     # Get GPU name\n",
    "print(torch.cuda.current_device())       # Current device ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91981,
     "status": "ok",
     "timestamp": 1717055713584,
     "user": {
      "displayName": "Alex Vandyke",
      "userId": "04540880566492736626"
     },
     "user_tz": -180
    },
    "id": "5dB2IPDz4NW_",
    "outputId": "cace9e5c-657b-4f1e-ece5-4db52e102c93"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 160\u001b[0m\n\u001b[0;32m    157\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m model \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_optimizer_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m optimizer \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lr_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(learning_rate) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_epochs_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(num_epochs) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_bs_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(batch_size) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_maxlen_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(max_len)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Fine-Tuning Phase\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mFinBertFineTuning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m classifier\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    165\u001b[0m classifier\u001b[38;5;241m.\u001b[39msave_model(absolute_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrainedModels/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m trained_model)\n",
      "Cell \u001b[1;32mIn[4], line 61\u001b[0m, in \u001b[0;36mFinBertFineTuning.__init__\u001b[1;34m(self, dataset_path, train_file, validation_file, feature_col, label_col, model_name, batch_size, learning_rate, num_epochs, max_len, optimizer, device)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mnum_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Optimizer setup\u001b[39;00m\n",
      "File \u001b[1;32mC:\\py_packages\\Python311\\site-packages\\transformers\\modeling_utils.py:311\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    313\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mC:\\py_packages\\Python311\\site-packages\\transformers\\modeling_utils.py:4839\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4830\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   4832\u001b[0m     (\n\u001b[0;32m   4833\u001b[0m         model,\n\u001b[0;32m   4834\u001b[0m         missing_keys,\n\u001b[0;32m   4835\u001b[0m         unexpected_keys,\n\u001b[0;32m   4836\u001b[0m         mismatched_keys,\n\u001b[0;32m   4837\u001b[0m         offload_index,\n\u001b[0;32m   4838\u001b[0m         error_msgs,\n\u001b[1;32m-> 4839\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4845\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4848\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4857\u001b[0m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[0;32m   4858\u001b[0m model\u001b[38;5;241m.\u001b[39m_tp_size \u001b[38;5;241m=\u001b[39m tp_size\n",
      "File \u001b[1;32mC:\\py_packages\\Python311\\site-packages\\transformers\\modeling_utils.py:5105\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[0;32m   5102\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m   5103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5104\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m-> 5105\u001b[0m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m   5106\u001b[0m     )\n\u001b[0;32m   5108\u001b[0m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[0;32m   5109\u001b[0m prefix \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbase_model_prefix\n",
      "File \u001b[1;32mC:\\py_packages\\Python311\\site-packages\\transformers\\modeling_utils.py:556\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m--> 556\u001b[0m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\py_packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1517\u001b[0m, in \u001b[0;36mcheck_torch_load_is_safe\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_torch_load_is_safe\u001b[39m():\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.6\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1518\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1519\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1520\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen loading files with safetensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1521\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1522\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SequenceClassificationDataset(Dataset): # Handle the input data and labels for PyTorch's DataLoader\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids']) # Return the total number of samples in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the input_ids, attention_mask, and label corresponding to the index\n",
    "        input_ids = self.inputs['input_ids'][idx]\n",
    "        attention_mask = self.inputs['attention_mask'][idx]\n",
    "        label = self.labels[idx]\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class FinBertFineTuning:\n",
    "    def __init__(self, dataset_path, train_file, validation_file, feature_col, label_col, model_name, batch_size, learning_rate, num_epochs, max_len, optimizer='AdamW', device='cpu'):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.train_file = train_file\n",
    "        self.validation_file = validation_file\n",
    "        self.feature_col = feature_col\n",
    "        self.label_col = label_col\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.max_len = max_len\n",
    "        self.optimizer = optimizer\n",
    "        self.device = torch.device(device)  # Convert device argument to torch.device\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, max_len=self.max_len)\n",
    "\n",
    "        # Load datasets\n",
    "        self.train_df = pd.read_csv(os.path.join(self.dataset_path, self.train_file))\n",
    "        self.validation_df = pd.read_csv(os.path.join(self.dataset_path, self.validation_file))\n",
    "\n",
    "        # Calculate number of unique labels\n",
    "        self.num_labels = len(self.train_df[self.label_col].unique())\n",
    "\n",
    "        # Tokenize datasets\n",
    "        self.tokenized_train = self.tokenize_dataset(self.train_df, self.feature_col, self.label_col)\n",
    "        self.tokenized_validation = self.tokenize_dataset(self.validation_df, self.feature_col, self.label_col)\n",
    "\n",
    "        # Model configuration\n",
    "        self.model_config = AutoConfig.from_pretrained(self.model_name, num_labels=self.num_labels)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.model_config)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Optimizer\n",
    "        if self.optimizer is None:\n",
    "            raise ValueError(\"Please provide an optimizer instance.\")\n",
    "\n",
    "        if self.optimizer == 'Adam':\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        elif self.optimizer == 'AdamW':\n",
    "            self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # DataLoaders\n",
    "        self.train_dataloader = self.create_dataloader(self.tokenized_train)\n",
    "        self.validation_dataloader = self.create_dataloader(self.tokenized_validation, shuffle=False)\n",
    "\n",
    "    def tokenize_dataset(self, df, feature_col, label_col):\n",
    "        return self.tokenizer(list(df[feature_col]),\n",
    "                              padding=True,\n",
    "                              truncation=True,\n",
    "                              return_tensors='pt'), list(df[label_col])\n",
    "\n",
    "    def create_dataloader(self, tokenized_dataset, shuffle=True):\n",
    "        dataset = SequenceClassificationDataset(tokenized_dataset[0], tokenized_dataset[1])\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle)\n",
    "\n",
    "    def evaluate_model(self, dataloader):\n",
    "        self.model.eval() # Set the model to evaluation mode\n",
    "        # Initialize lists to store true labels and predictions\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader: # Iterate over batches in the data loader\n",
    "                inputs = {key: value.to(self.device) for key, value in batch.items()} # Move inputs to the appropriate device (CPU or GPU)\n",
    "                labels = inputs[\"labels\"] # Extract labels from inputs\n",
    "                outputs = self.model(**inputs) # Forward pass through the model\n",
    "                logits = outputs.logits # Get logits from the model output\n",
    "\n",
    "                _, predicted = torch.max(logits, 1) # Compute predicted labels\n",
    "                # Convert labels and predictions to numpy arrays\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_predictions) # Calculate accuracy\n",
    "        return accuracy\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs): # Iterate over the num_epochs of epochs\n",
    "            self.model.train() # Set the model to training mode\n",
    "            train_losses = [] # List to store training losses for each batch\n",
    "\n",
    "            # Iterate over batches in the training data loader, displaying progress using tqdm\n",
    "            for batch in tqdm(self.train_dataloader, desc=f'Epoch {epoch + 1}/{self.num_epochs}'):\n",
    "                inputs = {key: value.to(self.device) for key, value in batch.items()} # Move inputs to the appropriate device (CPU or GPU)\n",
    "                outputs = self.model(**inputs) # Forward pass through the model\n",
    "                loss = outputs.loss # Retrieve the loss from the model output\n",
    "                train_losses.append(loss.item()) # Append the loss value to the list of training losses\n",
    "\n",
    "                self.optimizer.zero_grad() # Zero the gradients\n",
    "                loss.backward() # Backpropagate the gradients\n",
    "                self.optimizer.step() # Update the model parameters\n",
    "\n",
    "            # Validation\n",
    "            validation_losses = [] # Initialize an empty list to store validation losses\n",
    "            validation_accuracy = self.evaluate_model(self.validation_dataloader) # Evaluate model performance on the validation data loader\n",
    "\n",
    "            for batch in self.validation_dataloader:\n",
    "              inputs = {key: value.to(self.device) for key, value in batch.items()} # Move inputs to the appropriate device (CPU or GPU)\n",
    "              outputs = self.model(**inputs) # Forward pass through the model\n",
    "              loss = outputs.loss # Retrieve the loss from the model output\n",
    "              validation_losses.append(loss.item()) # Append the loss value to the list of validation losses\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{self.num_epochs} - Training Loss: {sum(train_losses) / len(train_losses):.4f} - Validation Loss: {sum(validation_losses) / len(validation_losses):.4f} - Validation Accuracy: {validation_accuracy:.4f}')\n",
    "\n",
    "    def save_model(self, directory):\n",
    "        self.model.save_pretrained(directory)\n",
    "        self.tokenizer.save_pretrained(directory)\n",
    "\n",
    "# Usage\n",
    "start_time = time.time()\n",
    "model = 'finbert'\n",
    "model_name = 'ProsusAI/finbert'\n",
    "\n",
    "## Hyperparameters\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 3\n",
    "batch_size = 6\n",
    "\n",
    "# Maximum sequence length for padding and truncation\n",
    "max_len = 512\n",
    "\n",
    "optimizer = 'Adam'  # Adam or AdamW\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Paths and filenames\n",
    "absolute_path = \"D:/Berkas/Code/CryptoNew/\"\n",
    "dataset_path = absolute_path + \"Datasets/\"\n",
    "train_file = 'train_set.csv'\n",
    "validation_file = 'validation_set.csv'\n",
    "feature_col = 'text'\n",
    "label_col = 'sentiment_numerical_fin'\n",
    "trained_model = model + '_optimizer_' + optimizer + '_lr_' + str(learning_rate) + '_epochs_' + str(\n",
    "    num_epochs) + '_bs_' + str(batch_size) + '_maxlen_' + str(max_len)\n",
    "\n",
    "# Fine-Tuning Phase\n",
    "classifier = FinBertFineTuning(dataset_path, train_file, validation_file, feature_col, label_col, model_name, batch_size,\n",
    "                             learning_rate, num_epochs, max_len, optimizer, device)\n",
    "classifier.train()\n",
    "classifier.save_model(absolute_path + 'TrainedModels/' + trained_model)\n",
    "print(\"Training time: {:.2f} seconds\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBtolfD5Y3Yh"
   },
   "source": [
    "## Use the Fine-tuned FinBERT model to make predictions for a specific test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49348,
     "status": "ok",
     "timestamp": 1715415084809,
     "user": {
      "displayName": "Roumeliotis Konstadinos",
      "userId": "17264923090131634662"
     },
     "user_tz": -180
    },
    "id": "Op_uIy1aGb_0",
    "outputId": "dc86a97f-7a24-4fab-f2e4-bf7132fca811"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "absolute_path = \"D:/Berkas/Code/CryptoNew/\"\n",
    "test_file = 'test_set.csv'\n",
    "trained_model_name = 'finbert_optimizer_Adam_lr_2e-05_epochs_3_bs_6_maxlen_512'\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(absolute_path, 'Datasets', test_file))\n",
    "\n",
    "# Load trained model and tokenizer\n",
    "model_path = os.path.join(absolute_path, 'TrainedModels', trained_model_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available, otherwise use CPU\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Tokenize test data\n",
    "tokenized_test = tokenizer(list(test_df['text']), padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = {key: value.to(device) for key, value in tokenized_test.items()}\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "test_df['finbert_adam_ft_prediction'] = predicted_labels.cpu().numpy()\n",
    "\n",
    "# Save the test dataset with predictions\n",
    "test_df.to_csv(os.path.join(absolute_path, 'Datasets', 'test_set_fin_adam.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.3460\n",
      "Precision:  0.3503\n",
      "Recall   :  0.3460\n",
      "F1 Score :  0.3481\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Path ke file hasil prediksi\n",
    "file_path = \"D:/Berkas/Code/CryptoNew/Datasets/test_set_fin_adam.csv\"\n",
    "\n",
    "# Baca file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ambil label asli dan prediksi\n",
    "y_true = df['sentiment_numerical']\n",
    "y_pred = df['finbert_adam_ft_prediction']\n",
    "\n",
    "# Hitung metrik\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(f\"Accuracy :  {accuracy:.4f}\")\n",
    "print(f\"Precision:  {precision:.4f}\")\n",
    "print(f\"Recall   :  {recall:.4f}\")\n",
    "print(f\"F1 Score :  {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
