{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3fb4f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\py_packages\\python311\\site-packages (4.53.2)\n",
      "Requirement already satisfied: datasets in c:\\py_packages\\python311\\site-packages (3.5.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\py_packages\\python311\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pandas in c:\\py_packages\\python311\\site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in c:\\py_packages\\python311\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\py_packages\\python311\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\py_packages\\python311\\site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\py_packages\\python311\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\py_packages\\python311\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\py_packages\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\py_packages\\python311\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\py_packages\\python311\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\py_packages\\python311\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\py_packages\\python311\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\py_packages\\python311\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\py_packages\\python311\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\py_packages\\python311\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\py_packages\\python311\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\py_packages\\python311\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\py_packages\\python311\\site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\py_packages\\python311\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\py_packages\\python311\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\py_packages\\python311\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\py_packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\py_packages\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\py_packages\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: colorama in c:\\py_packages\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\py_packages\\python311\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\py_packages\\python311\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\py_packages\\python311\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\py_packages\\python311\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\py_packages\\python311\\site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\py_packages\\python311\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\py_packages\\python311\\site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\py_packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\py_packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\py_packages\\python311\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\py_packages\\python311\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\py_packages\\python311\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\py_packages\\python311\\site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\py_packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\py_packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\py_packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\py_packages\\Python311\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\IKBAR\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets scikit-learn pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4ad4728",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RobertaForSequenceClassification' from 'transformers' (C:\\py_packages\\Python311\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaTokenizer, RobertaForSequenceClassification, RobertaConfig\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdamW\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'RobertaForSequenceClassification' from 'transformers' (C:\\py_packages\\Python311\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaConfig\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SequenceClassificationDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs['input_ids'][idx],\n",
    "            'attention_mask': self.inputs['attention_mask'][idx],\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class RobertaFineTuning:\n",
    "    def __init__(self, dataset_path, train_file, validation_file, feature_col, label_col, model_name, batch_size, learning_rate, num_epochs, max_len, optimizer='AdamW', device='cpu'):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.train_file = train_file\n",
    "        self.validation_file = validation_file\n",
    "        self.feature_col = feature_col\n",
    "        self.label_col = label_col\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.max_len = max_len\n",
    "        self.optimizer_type = optimizer\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        self.train_df = pd.read_csv(os.path.join(self.dataset_path, self.train_file))\n",
    "        self.validation_df = pd.read_csv(os.path.join(self.dataset_path, self.validation_file))\n",
    "\n",
    "        self.num_labels = len(self.train_df[self.label_col].unique())\n",
    "\n",
    "        self.tokenized_train = self.tokenize_dataset(self.train_df, self.feature_col, self.label_col)\n",
    "        self.tokenized_validation = self.tokenize_dataset(self.validation_df, self.feature_col, self.label_col)\n",
    "\n",
    "        self.model_config = RobertaConfig.from_pretrained(self.model_name, num_labels=self.num_labels)\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(self.model_name, config=self.model_config)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        if self.optimizer_type == 'Adam':\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        elif self.optimizer_type == 'AdamW':\n",
    "            self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer type. Use 'Adam' or 'AdamW'.\")\n",
    "\n",
    "        self.train_dataloader = self.create_dataloader(self.tokenized_train)\n",
    "        self.validation_dataloader = self.create_dataloader(self.tokenized_validation, shuffle=False)\n",
    "\n",
    "    def tokenize_dataset(self, df, feature_col, label_col):\n",
    "        tokens = self.tokenizer(\n",
    "            list(df[feature_col]),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return tokens, list(df[label_col])\n",
    "\n",
    "    def create_dataloader(self, tokenized_dataset, shuffle=True):\n",
    "        dataset = SequenceClassificationDataset(tokenized_dataset[0], tokenized_dataset[1])\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle)\n",
    "\n",
    "    def evaluate_model(self, dataloader):\n",
    "        self.model.eval()\n",
    "        all_labels, all_predictions = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                inputs = {key: value.to(self.device) for key, value in batch.items()}\n",
    "                labels = inputs[\"labels\"]\n",
    "                outputs = self.model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "                logits = outputs.logits\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "        recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            train_losses = []\n",
    "\n",
    "            for batch in tqdm(self.train_dataloader, desc=f'Epoch {epoch + 1}/{self.num_epochs}'):\n",
    "                inputs = {key: value.to(self.device) for key, value in batch.items()}\n",
    "                outputs = self.model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=inputs[\"labels\"])\n",
    "                loss = outputs.loss\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            validation_losses = []\n",
    "            val_acc, val_prec, val_rec, val_f1 = self.evaluate_model(self.validation_dataloader)\n",
    "\n",
    "            for batch in self.validation_dataloader:\n",
    "                inputs = {key: value.to(self.device) for key, value in batch.items()}\n",
    "                outputs = self.model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=inputs[\"labels\"])\n",
    "                validation_losses.append(outputs.loss.item())\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs} - Train Loss: {sum(train_losses)/len(train_losses):.4f} - Val Loss: {sum(validation_losses)/len(validation_losses):.4f} - Val Acc: {val_acc:.4f} - Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    def save_model(self, directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        self.model.save_pretrained(directory)\n",
    "        self.tokenizer.save_pretrained(directory)\n",
    "\n",
    "# === MAIN EKSEKUSI ===\n",
    "start_time = time.time()\n",
    "model = 'roberta'\n",
    "model_name = 'roberta-base'\n",
    "\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 3\n",
    "batch_size = 6\n",
    "max_len = 512\n",
    "optimizer = 'AdamW'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "absolute_path = \"D:/Berkas/Code/CryptoNew/\"\n",
    "dataset_path = absolute_path + \"Datasets/\"\n",
    "train_file = 'train_set.csv'\n",
    "validation_file = 'validation_set.csv'\n",
    "feature_col = 'text'\n",
    "label_col = 'sentiment_numerical'\n",
    "trained_model = model + '_optimizer_' + optimizer + '_lr_' + str(learning_rate) + '_epochs_' + str(num_epochs) + '_bs_' + str(batch_size) + '_maxlen_' + str(max_len)\n",
    "\n",
    "classifier = RobertaFineTuning(dataset_path, train_file, validation_file, feature_col, label_col, model_name, batch_size, learning_rate, num_epochs, max_len, optimizer, device)\n",
    "\n",
    "classifier.train()\n",
    "classifier.save_model(absolute_path + 'TrainedModels/' + trained_model)\n",
    "\n",
    "print(\"Training time: {:.2f} seconds\".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d15bf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediksi selesai disimpan ke test_set_roberta_optimizer_AdamW_lr_3e-05_epochs_3_bs_6_maxlen_512.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# === KONFIGURASI ===\n",
    "absolute_path = \"D:/Berkas/Code/CryptoNew/\"\n",
    "test_file = 'test_set.csv'\n",
    "trained_model_name = 'roberta_optimizer_AdamW_lr_2e-05_epochs_3_bs_6_maxlen_512'  # GANTI nama folder model sesuai modelmu\n",
    "\n",
    "# Baca data uji\n",
    "test_df = pd.read_csv(os.path.join(absolute_path, 'Datasets', test_file))\n",
    "\n",
    "# Load model & tokenizer dari folder hasil fine-tuning\n",
    "model_path = os.path.join(absolute_path, 'TrainedModels', trained_model_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Tokenisasi teks uji\n",
    "tokenized_test = tokenizer(\n",
    "    list(test_df['text']),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Pindahkan token ke device (GPU/CPU)\n",
    "tokenized_test = {k: v.to(device) for k, v in tokenized_test.items()}\n",
    "\n",
    "# Prediksi\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized_test)\n",
    "    logits = outputs.logits\n",
    "    _, predicted_labels = torch.max(logits, dim=1)\n",
    "\n",
    "# Tambahkan kolom prediksi ke DataFrame\n",
    "output_col = trained_model_name + '_prediction'\n",
    "test_df[output_col] = predicted_labels.cpu().numpy()\n",
    "\n",
    "# Simpan ke file\n",
    "output_file = f'test_set_{trained_model_name}.csv'\n",
    "test_df.to_csv(os.path.join(absolute_path, 'Datasets', output_file), index=False)\n",
    "\n",
    "print(f\"Prediksi selesai disimpan ke {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c988a377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.3330\n",
      "Precision:  0.1109\n",
      "Recall   :  0.3330\n",
      "F1 Score :  0.1664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\py_packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Path ke file hasil prediksi\n",
    "file_path = \"D:/Berkas/Code/CryptoNew/Datasets/test_set_roberta_optimizer_AdamW_lr_2e-05_epochs_3_bs_6_maxlen_512.csv\"\n",
    "\n",
    "# Baca file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ambil label asli dan prediksi\n",
    "y_true = df['sentiment_numerical']\n",
    "y_pred = df['roberta_optimizer_AdamW_lr_3e-05_epochs_3_bs_6_maxlen_512_prediction']\n",
    "\n",
    "# Hitung metrik\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(f\"Accuracy :  {accuracy:.4f}\")\n",
    "print(f\"Precision:  {precision:.4f}\")\n",
    "print(f\"Recall   :  {recall:.4f}\")\n",
    "print(f\"F1 Score :  {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
