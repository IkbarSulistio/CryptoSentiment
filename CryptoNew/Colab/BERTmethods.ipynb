{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeJ9KDW4L67C"
   },
   "source": [
    "# Fine-tune BERT to make predictions based on specific train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75168,
     "status": "ok",
     "timestamp": 1716974539111,
     "user": {
      "displayName": "Alex Vandyke",
      "userId": "04540880566492736626"
     },
     "user_tz": -180
    },
    "id": "_zoS8wgsLxts",
    "outputId": "804b2ced-a8f1-4c6a-c66b-bee470868382"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\py_packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████| 534/534 [01:24<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training Loss: 0.9533 - Validation Loss: 0.6748 - Validation Accuracy: 0.7388 - Validation Precision: 0.7422 - Validation Recall: 0.7388 - Validation F1 Score: 0.7383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 534/534 [01:33<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training Loss: 0.4664 - Validation Loss: 0.4737 - Validation Accuracy: 0.8250 - Validation Precision: 0.8287 - Validation Recall: 0.8250 - Validation F1 Score: 0.8241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 534/534 [01:38<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training Loss: 0.2452 - Validation Loss: 0.4728 - Validation Accuracy: 0.8512 - Validation Precision: 0.8527 - Validation Recall: 0.8512 - Validation F1 Score: 0.8514\n",
      "Training time: 313.03 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SequenceClassificationDataset(Dataset): # Handle the input data and labels for PyTorch's DataLoader\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids']) # Return the total number of samples in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the input_ids, attention_mask, and label corresponding to the index\n",
    "        input_ids = self.inputs['input_ids'][idx]\n",
    "        attention_mask = self.inputs['attention_mask'][idx]\n",
    "        label = self.labels[idx]\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class BertFineTuning:\n",
    "    def __init__(self, dataset_path, train_file, validation_file, feature_col, label_col, model_name, batch_size, learning_rate, num_epochs, max_len, optimizer=None, device='cpu'):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.train_file = train_file\n",
    "        self.validation_file = validation_file\n",
    "        self.feature_col = feature_col\n",
    "        self.label_col = label_col\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.max_len = max_len\n",
    "        self.optimizer = optimizer\n",
    "        self.device = torch.device(device)  # Convert device argument to torch.device\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name, max_len=self.max_len)\n",
    "\n",
    "        # Load datasets\n",
    "        self.train_df = pd.read_csv(os.path.join(self.dataset_path, self.train_file))\n",
    "        self.validation_df = pd.read_csv(os.path.join(self.dataset_path, self.validation_file))\n",
    "\n",
    "        # Calculate number of unique labels\n",
    "        self.num_labels = len(self.train_df[self.label_col].unique())\n",
    "\n",
    "        # Tokenize datasets\n",
    "        self.tokenized_train = self.tokenize_dataset(self.train_df, self.feature_col, self.label_col)\n",
    "        self.tokenized_validation = self.tokenize_dataset(self.validation_df, self.feature_col, self.label_col)\n",
    "\n",
    "        # Model configuration\n",
    "        self.model_config = BertConfig.from_pretrained(self.model_name, num_labels=self.num_labels)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(self.model_name, config=self.model_config)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Optimizer\n",
    "        if self.optimizer is None:\n",
    "            raise ValueError(\"Please provide an optimizer instance.\")\n",
    "\n",
    "        if self.optimizer == 'Adam':\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        elif self.optimizer == 'AdamW':\n",
    "            self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # DataLoaders\n",
    "        self.train_dataloader = self.create_dataloader(self.tokenized_train)\n",
    "        self.validation_dataloader = self.create_dataloader(self.tokenized_validation, shuffle=False)\n",
    "\n",
    "    def tokenize_dataset(self, df, feature_col, label_col):\n",
    "        return self.tokenizer(list(df[feature_col]),\n",
    "                              padding=True,\n",
    "                              truncation=True,\n",
    "                              return_tensors='pt'), list(df[label_col])\n",
    "\n",
    "    def create_dataloader(self, tokenized_dataset, shuffle=True):\n",
    "        dataset = SequenceClassificationDataset(tokenized_dataset[0], tokenized_dataset[1])\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle)\n",
    "\n",
    "    def evaluate_model(self, dataloader):\n",
    "        self.model.eval() # Set the model to evaluation mode\n",
    "        # Initialize lists to store true labels and predictions\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader: # Iterate over batches in the data loader\n",
    "                inputs = {key: value.to(self.device) for key, value in batch.items()} # Move inputs to the appropriate device (CPU or GPU)\n",
    "                labels = inputs[\"labels\"] # Extract labels from inputs\n",
    "                outputs = self.model(**inputs) # Forward pass through the model\n",
    "                logits = outputs.logits # Get logits from the model output\n",
    "\n",
    "                _, predicted = torch.max(logits, 1) # Compute predicted labels\n",
    "                # Convert labels and predictions to numpy arrays\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "        recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs): # Iterate over the num_epochs of epochs\n",
    "            self.model.train() # Set the model to training mode\n",
    "            train_losses = [] # List to store training losses for each batch\n",
    "\n",
    "            # Iterate over batches in the training data loader, displaying progress using tqdm\n",
    "            for batch in tqdm(self.train_dataloader, desc=f'Epoch {epoch + 1}/{self.num_epochs}'):\n",
    "                inputs = {key: value.to(self.device) for key, value in batch.items()} # Move inputs to the appropriate device (CPU or GPU)\n",
    "                outputs = self.model(**inputs) # Forward pass through the model\n",
    "                loss = outputs.loss # Retrieve the loss from the model output\n",
    "                train_losses.append(loss.item()) # Append the loss value to the list of training losses\n",
    "\n",
    "                self.optimizer.zero_grad() # Zero the gradients\n",
    "                loss.backward() # Backpropagate the gradients\n",
    "                self.optimizer.step() # Update the model parameters\n",
    "\n",
    "            # Validation\n",
    "            validation_losses = [] # Initialize an empty list to store validation losses\n",
    "            validation_accuracy, validation_precision, validation_recall, validation_f1 = self.evaluate_model(self.validation_dataloader) # Evaluate model performance on the validation data loader\n",
    "\n",
    "            for batch in self.validation_dataloader:\n",
    "              inputs = {key: value.to(self.device) for key, value in batch.items()} # Move inputs to the appropriate device (CPU or GPU)\n",
    "              outputs = self.model(**inputs) # Forward pass through the model\n",
    "              loss = outputs.loss # Retrieve the loss from the model output\n",
    "              validation_losses.append(loss.item()) # Append the loss value to the list of validation losses\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{self.num_epochs} - Training Loss: {sum(train_losses) / len(train_losses):.4f} - Validation Loss: {sum(validation_losses) / len(validation_losses):.4f} - Validation Accuracy: {validation_accuracy:.4f} - Validation Precision: {validation_precision:.4f} - Validation Recall: {validation_recall:.4f} - Validation F1 Score: {validation_f1:.4f}')\n",
    "\n",
    "    def save_model(self, directory):\n",
    "        self.model.save_pretrained(directory)\n",
    "        self.tokenizer.save_pretrained(directory)\n",
    "\n",
    "# Usage\n",
    "start_time = time.time()\n",
    "model = 'bert'\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "## Hyperparameters\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 3\n",
    "batch_size = 6\n",
    "max_len = 512\n",
    "\n",
    "optimizer = 'Adam' # Adam or AdamW\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # or device = 'cpu'\n",
    "\n",
    "## Paths and filenames\n",
    "absolute_path = \"D:/Berkas/Code/CryptoNew/\"\n",
    "dataset_path = absolute_path + \"Datasets/\"\n",
    "train_file = 'train_set.csv'\n",
    "validation_file = 'validation_set.csv'\n",
    "feature_col = 'text'\n",
    "label_col = 'sentiment_numerical'\n",
    "trained_model = model + '_optimizer_' + optimizer + '_lr_' + str(learning_rate) + '_epochs_' + str(num_epochs) + '_bs_' + str(batch_size) + '_maxlen_' + str(max_len)\n",
    "\n",
    "# Fine-Tuning Phase\n",
    "classifier = BertFineTuning(dataset_path, train_file, validation_file, feature_col, label_col, model_name, batch_size, learning_rate, num_epochs, max_len, optimizer, device)\n",
    "classifier.train()\n",
    "classifier.save_model(absolute_path + 'TrainedModels/' + trained_model)\n",
    "print(\"Training time: {:.2f} seconds\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkgyBo4QMF7N"
   },
   "source": [
    "# Use the Fine-tuned BERT model to make predictions for a specific test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14750,
     "status": "ok",
     "timestamp": 1716974896382,
     "user": {
      "displayName": "Alex Vandyke",
      "userId": "04540880566492736626"
     },
     "user_tz": -180
    },
    "id": "KXzJPPvDMILg",
    "outputId": "ed6f2175-a411-446a-d333-5abdbb8c11ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 90.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BertPredictions:\n",
    "    def __init__(self, model_path, device, max_len):\n",
    "        self.model_path = model_path\n",
    "        self.max_len = max_len\n",
    "        self.device = torch.device(device)  # Convert device argument to torch.device\n",
    "        self.model, self.tokenizer = self.load_fine_tuned_bert_model()\n",
    "\n",
    "    def load_fine_tuned_bert_model(self):\n",
    "        model = BertForSequenceClassification.from_pretrained(self.model_path) # Load the fine-tuned BERT model\n",
    "        tokenizer = BertTokenizer.from_pretrained(self.model_path) # Load the tokenizer\n",
    "        model.to(self.device) # Move the model to the specified device\n",
    "        return model, tokenizer\n",
    "\n",
    "    def predict(self, input):\n",
    "        tokens = self.tokenizer.tokenize(self.tokenizer.decode(self.tokenizer.encode(input))) # Tokenize the input using the loaded tokenizer\n",
    "\n",
    "        # Truncate the tokens if the length exceeds max_len - 2\n",
    "        if len(tokens) > self.max_len - 2:\n",
    "            tokens = tokens[:self.max_len - 2]\n",
    "\n",
    "        # Encode the tokens and convert them to PyTorch tensor\n",
    "        input_ids = self.tokenizer.encode(tokens, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.model.eval() # Set the model to evaluation mode\n",
    "            logits = self.model(input_ids)[0] # Perform forward pass through the model\n",
    "            predictions = torch.argmax(logits, dim=1).item() # Predict the label by selecting the index with the highest logit value\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def predict_and_save(self, dataset_path, test_file, feature_col, prediction_col):\n",
    "        # Load the test dataset\n",
    "        test_df = pd.read_csv(os.path.join(dataset_path, test_file))\n",
    "\n",
    "        # Backup the original file by renaming it\n",
    "        os.rename(os.path.join(dataset_path, test_file), os.path.join(dataset_path, 'test_set_original.csv'))\n",
    "\n",
    "        # Iterate through each row in the DataFrame\n",
    "        for index, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "            content = row[feature_col]\n",
    "\n",
    "            # Process the content and predict the label\n",
    "            predicted_rating = self.predict(content)\n",
    "\n",
    "            # Update the prediction_col column\n",
    "            test_df.at[index, prediction_col] = predicted_rating\n",
    "\n",
    "        # Save results to CSV\n",
    "        test_df.to_csv(os.path.join(dataset_path, test_file), index=False)\n",
    "\n",
    "# Usage\n",
    "max_len = 512\n",
    "\n",
    "str_params = 'bert_optimizer_Adam_lr_2e-05_epochs_3_bs_6_maxlen_512'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Determine device\n",
    "optimizer = \"Adam\"  # Set the correct optimizer\n",
    "\n",
    "## Paths and filenames\n",
    "path = \"D:/Berkas/Code/CryptoNew/\"\n",
    "dataset_path = path + \"Datasets/\"\n",
    "test_file = \"test_set.csv\"\n",
    "trained_model = path + 'TrainedModels/' + str_params  # The fine-tuned model\n",
    "feature_col = 'text'\n",
    "# prediction_col = str_params + '_prediction'\n",
    "prediction_col = 'bert_adam_ft_prediction_new2'\n",
    "\n",
    "# Instantiate the BertPredictions class\n",
    "prediction = BertPredictions(trained_model, device, max_len)\n",
    "\n",
    "# Run prediction and save results to CSV\n",
    "prediction.predict_and_save(dataset_path, test_file, feature_col, prediction_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8010\n",
      "Precision:  0.8159\n",
      "Recall   :  0.8010\n",
      "F1 Score :  0.8020\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Path ke file hasil prediksi\n",
    "file_path = \"D:/Berkas/Code/CryptoNew/Datasets/test_set.csv\"\n",
    "\n",
    "# Baca file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ambil label asli dan prediksi\n",
    "y_true = df['sentiment_numerical']\n",
    "y_pred = df['bert_adamw_ft_prediction']\n",
    "\n",
    "# Hitung metrik\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(f\"Accuracy :  {accuracy:.4f}\")\n",
    "print(f\"Precision:  {precision:.4f}\")\n",
    "print(f\"Recall   :  {recall:.4f}\")\n",
    "print(f\"F1 Score :  {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
